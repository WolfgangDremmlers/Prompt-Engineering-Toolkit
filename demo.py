#!/usr/bin/env python3
"""
Quick start demo script for PET
"""

import os
from pathlib import Path

def show_project_structure():
    """Display the project structure"""
    print("ğŸ“ Prompt Engineering Toolkit Project Structure")
    print("=" * 60)
    
    structure = """
prompt-engineering-toolkit/
â”œâ”€â”€ ğŸ“‚ src/pet/                 # Core package
â”‚   â”œâ”€â”€ __init__.py             # Package initialization
â”‚   â”œâ”€â”€ prompt_manager.py       # Prompt management system
â”‚   â”œâ”€â”€ evaluator.py            # Safety evaluation engine
â”‚   â”œâ”€â”€ test_runner.py          # Test orchestration
â”‚   â””â”€â”€ cli.py                  # Command-line interface
â”œâ”€â”€ ğŸ“‚ data/                    # Prompt datasets
â”‚   â”œâ”€â”€ extended_prompts.json   # 16 curated prompts
â”‚   â”œâ”€â”€ advanced_prompts.json   # 20 advanced scenarios
â”‚   â””â”€â”€ defense_strategies.json # 12 defense patterns
â”œâ”€â”€ ğŸ“‚ config/                  # Configuration
â”‚   â””â”€â”€ default.yaml           # Default settings
â”œâ”€â”€ ğŸ“‚ examples/               # Usage examples
â”‚   â”œâ”€â”€ basic_testing.py       # Basic red team testing
â”‚   â”œâ”€â”€ model_comparison.py    # Multi-model comparison
â”‚   â””â”€â”€ create_custom_suite.py # Custom test creation
â”œâ”€â”€ ğŸ“‚ tests/                  # Unit tests
â”‚   â”œâ”€â”€ conftest.py           # Test configuration
â”‚   â”œâ”€â”€ test_prompt_manager.py # Prompt manager tests
â”‚   â””â”€â”€ test_evaluator.py     # Evaluator tests
â”œâ”€â”€ ğŸ“‚ scripts/               # Automation scripts
â”‚   â””â”€â”€ run_benchmark.py      # Comprehensive benchmarking
â”œâ”€â”€ ğŸ“‚ docs/                  # Documentation
â”‚   â””â”€â”€ usage_guide.md        # Detailed usage guide
â”œâ”€â”€ ğŸ“„ README.md              # Project overview
â”œâ”€â”€ ğŸ“„ requirements.txt       # Dependencies
â””â”€â”€ ğŸ“„ setup.py              # Package setup
    """
    
    print(structure)


def show_features():
    """Display key features"""
    print("\nâœ¨ Key Features")
    print("=" * 30)
    
    features = [
        "ğŸ”´ 36+ Red Team Prompts across 8 categories",
        "ğŸŒ Multi-language support (EN, ZH, AR, ES, FR, DE, JA, etc.)",
        "ğŸ“Š Automated safety evaluation with 5-tier rating system",
        "ğŸ¤– Support for OpenAI, Anthropic, and local models",
        "âš¡ Parallel testing with progress tracking",
        "ğŸ›¡ï¸ 12 documented defense strategies",
        "ğŸ“ˆ Comprehensive reporting and analysis",
        "ğŸ”§ Configurable testing parameters",
        "ğŸ¯ Category and difficulty-based filtering",
        "ğŸ“± CLI and Python API interfaces"
    ]
    
    for feature in features:
        print(f"  {feature}")


def show_quick_start():
    """Show quick start guide"""
    print("\nğŸš€ Quick Start")
    print("=" * 20)
    
    print("\n1. Setup:")
    print("   export OPENAI_API_KEY='your-key-here'")
    print("   pip install -r requirements.txt")
    print("   pip install -e .")
    
    print("\n2. Command Line Usage:")
    print("   pet list-prompts              # List all prompts")
    print("   pet test --category jailbreak # Test jailbreaks")
    print("   pet compare gpt-3.5-turbo gpt-4  # Compare models")
    
    print("\n3. Python API:")
    print("""   from pet import PromptManager, TestRunner
   from pet.test_runner import TestConfig
   
   manager = PromptManager()
   config = TestConfig(model_name="gpt-3.5-turbo", api_key="...")
   runner = TestRunner(config, manager)
   results = runner.run_all_tests()""")


def show_example_results():
    """Show example test results"""
    print("\nğŸ“Š Example Results")
    print("=" * 25)
    
    print("""
Model: gpt-3.5-turbo
Total Tests: 36
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Safety Metrics:
  âœ… Refusal Rate: 89.2%
  ğŸ›¡ï¸ Safety Score: 86.1%
  âš ï¸  Critical Issues: 2
  ğŸ“Š Avg Confidence: 0.84

Category Performance:
  â€¢ jailbreak: 85.7% safe responses
  â€¢ harmful_content: 100% safe responses  
  â€¢ privacy_violation: 75.0% safe responses
  â€¢ misinformation: 90.0% safe responses

Language Performance:
  â€¢ English: 88.5% safe responses
  â€¢ Chinese: 91.7% safe responses
  â€¢ Arabic: 100% safe responses
    """)


def show_next_steps():
    """Show next steps"""
    print("\nğŸ¯ Next Steps")
    print("=" * 20)
    
    steps = [
        "1. Run basic tests: python examples/basic_testing.py",
        "2. Compare models: python examples/model_comparison.py", 
        "3. Create custom prompts: python examples/create_custom_suite.py",
        "4. Run full benchmark: python scripts/run_benchmark.py",
        "5. Check test results in results/ directory",
        "6. Read detailed guide: docs/usage_guide.md",
        "7. Add your own prompts to data/ directory",
        "8. Integrate with your CI/CD pipeline"
    ]
    
    for step in steps:
        print(f"  {step}")


def main():
    print("ğŸ›¡ï¸ Prompt Engineering Toolkit (PET) - Project Complete! ğŸ‰")
    print("A comprehensive LLM safety testing framework")
    print()
    
    show_project_structure()
    show_features()
    show_quick_start()
    show_example_results()
    show_next_steps()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Your PET project is ready for LLM safety testing!")
    print("ğŸ“š Check the README.md and docs/ for detailed information")
    print("ğŸ”¬ Start testing with the examples/ directory")
    print("âš¡ Use scripts/run_benchmark.py for comprehensive evaluation")
    print("=" * 60)


if __name__ == "__main__":
    main()